{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34902b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from typing import Dict, Set, List,TypedDict\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader, PyPDFLoader, Docx2txtLoader\n",
    "from pydantic import BaseModel,Field\n",
    "from langgraph.graph import StateGraph,END,START\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from prompt import multiquery_or_ambigious_system_prompt,retrieval_argumented_generation_system_prompt, category_classification_system_prompt,multiquery_splitter_system_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e956cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------- CONFIG ---------------- #\n",
    "BASE_DIR = \"./collections\"     # put subfolders here (each subfolder = one collection)\n",
    "INDEX_DIR = \"./faiss_indexes\"  # where per-collection FAISS indexes will be saved\n",
    "EMBEDDINGS = OpenAIEmbeddings(model='text-embedding-3-small', api_key=\"\")\n",
    "CHUNK_SIZE = 1000\n",
    "CHUNK_OVERLAP = 200\n",
    "# ---------------------------------------- #\n",
    "\n",
    "\n",
    "def discover_collections(base_dir: str) -> List[str]:\n",
    "    \"\"\"Return sorted list of subfolder names under base_dir (each becomes a collection).\"\"\"\n",
    "    os.makedirs(base_dir, exist_ok=True)\n",
    "    names = [\n",
    "        name for name in os.listdir(base_dir)\n",
    "        if os.path.isdir(os.path.join(base_dir, name)) and not name.startswith(\".\")\n",
    "    ]\n",
    "    return sorted(names)\n",
    "\n",
    "\n",
    "def get_index_path(collection_name: str) -> str:\n",
    "    return os.path.join(INDEX_DIR, f\"{collection_name}_index\")\n",
    "\n",
    "\n",
    "def get_metadata_path(collection_name: str) -> str:\n",
    "    return os.path.join(INDEX_DIR, f\"{collection_name}_processed.pkl\")\n",
    "\n",
    "\n",
    "def load_processed_files(collection_name: str) -> Set[str]:\n",
    "    path = get_metadata_path(collection_name)\n",
    "    if os.path.exists(path):\n",
    "        with open(path, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "    return set()\n",
    "\n",
    "\n",
    "def save_processed_files(collection_name: str, processed_files: Set[str]):\n",
    "    path = get_metadata_path(collection_name)\n",
    "    with open(path, \"wb\") as f:\n",
    "        pickle.dump(processed_files, f)\n",
    "\n",
    "\n",
    "def split_docs(docs):\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
    "    return splitter.split_documents(docs)\n",
    "\n",
    "\n",
    "def load_documents_from_folder(folder_path, file_names):\n",
    "    \"\"\"Load documents from the list of file names (supports .txt, .pdf, .docx).\"\"\"\n",
    "    docs = []\n",
    "    for file in file_names:\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        ext = file.lower().split(\".\")[-1]\n",
    "\n",
    "        try:\n",
    "            if ext == \"txt\":\n",
    "                loader = TextLoader(file_path, encoding=\"utf-8\")\n",
    "            elif ext == \"pdf\":\n",
    "                loader = PyPDFLoader(file_path)\n",
    "            elif ext in [\"docx\", \"doc\"]:\n",
    "                loader = Docx2txtLoader(file_path)\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è Skipping unsupported file type: {file}\")\n",
    "                continue\n",
    "\n",
    "            docs.extend(loader.load())\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to load {file}: {e}\")\n",
    "    return docs\n",
    "\n",
    "\n",
    "def build_or_update_collection(collection_name: str):\n",
    "    \"\"\"Create or update FAISS index for a collection folder.\"\"\"\n",
    "    folder_path = os.path.join(BASE_DIR, collection_name)\n",
    "    index_path = get_index_path(collection_name)\n",
    "\n",
    "    os.makedirs(INDEX_DIR, exist_ok=True)\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "    processed_files = load_processed_files(collection_name)\n",
    "\n",
    "    # detect new files (supported extensions)\n",
    "    all_files = {\n",
    "        f for f in os.listdir(folder_path)\n",
    "        if f.lower().endswith((\".txt\", \".pdf\", \".docx\"))\n",
    "    }\n",
    "    new_files = all_files - processed_files\n",
    "\n",
    "    vectordb = None\n",
    "    if os.path.exists(index_path):\n",
    "        # load existing FAISS index\n",
    "        try:\n",
    "            print(f\"Loading existing FAISS index for collection: {collection_name}\")\n",
    "            vectordb = FAISS.load_local(index_path, EMBEDDINGS, allow_dangerous_deserialization=True)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Failed to load existing index for {collection_name}: {e}. Will try to recreate if there are docs.\")\n",
    "\n",
    "    if new_files:\n",
    "        print(f\"üìÇ Found new files in {collection_name}: {new_files}\")\n",
    "        new_docs = load_documents_from_folder(folder_path, new_files)\n",
    "        chunks = split_docs(new_docs)\n",
    "\n",
    "        if chunks:\n",
    "            if vectordb:\n",
    "                vectordb.add_documents(chunks)\n",
    "            else:\n",
    "                vectordb = FAISS.from_documents(chunks, EMBEDDINGS)\n",
    "\n",
    "            try:\n",
    "                vectordb.save_local(index_path)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error saving index for {collection_name}: {e}\")\n",
    "\n",
    "            processed_files.update(new_files)\n",
    "            save_processed_files(collection_name, processed_files)\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è No valid text chunks found in new files for {collection_name}\")\n",
    "\n",
    "    if vectordb is None:\n",
    "        # No index and no new docs -> return None (safe handling by caller)\n",
    "        print(f\"‚ö†Ô∏è No documents for {collection_name}. Skipping index creation.\")\n",
    "        return None\n",
    "\n",
    "    return vectordb\n",
    "\n",
    "\n",
    "def query_collection(dbs, collection_name: str, query: str, k=3):\n",
    "    \"\"\"Safely query a collection. Returns list of Documents or empty list.\"\"\"\n",
    "    db = dbs.get(collection_name)\n",
    "    if db is None:\n",
    "        print(f\"‚ö†Ô∏è No index found for '{collection_name}'. Add documents to '{os.path.join(BASE_DIR, collection_name)}' and re-run.\")\n",
    "        return []\n",
    "    return db.similarity_search(query, k=k)\n",
    "\n",
    "\n",
    "def main(query : str , sample_collection : str):\n",
    "\n",
    "    # discover collection folders dynamically\n",
    "    collections = discover_collections(BASE_DIR)\n",
    "\n",
    "    if not collections:\n",
    "\n",
    "        return f\"‚ÑπÔ∏è No collections found in '{BASE_DIR}'. Create subfolders there (one per collection), then run again.\"\n",
    "    else:\n",
    "        \n",
    "        print(f\"Discovered collections: {collections}\")\n",
    "\n",
    "    dbs: Dict[str, FAISS] = {}\n",
    "\n",
    "    print(\"\\nüîç Building/Updating FAISS Indexes...\\n\")\n",
    "\n",
    "    for col in collections:\n",
    "\n",
    "        dbs[col] = build_or_update_collection(col)\n",
    "\n",
    "    print(\"\\n‚úÖ Summary of collections:\")\n",
    "\n",
    "    for col in collections:\n",
    "\n",
    "        db = dbs.get(col)\n",
    "\n",
    "        if db is None:\n",
    "\n",
    "            print(f\" - {col}: 0 documents indexed\")\n",
    "\n",
    "        else:\n",
    "\n",
    "            try:\n",
    "\n",
    "                count = db.index.ntotal\n",
    "\n",
    "            except Exception:\n",
    "\n",
    "                count = \"unknown\"\n",
    "\n",
    "            print(f\" - {col}: {count} documents indexed\")\n",
    "\n",
    "    if sample_collection:\n",
    "\n",
    "        results = query_collection(dbs, sample_collection, query, k=3)\n",
    "\n",
    "        print(f\"......{len(results)} OF CHUNKS RETRIEVED FOR {query}\")\n",
    "\n",
    "        if results:\n",
    "\n",
    "            for r in results:\n",
    "\n",
    "                return  f\"{r.page_content[:300].replace('\\n', ' ')}\"\n",
    "            \n",
    "        else:\n",
    "\n",
    "            return f\"\\n‚ö†Ô∏è No results found for {sample_collection}.\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed7a967",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_2516\\1174643080.py:18: DeprecationWarning: Retrievers must implement abstract `_get_relevant_documents` method instead of `get_relevant_documents`\n",
      "  class DummyRetriever(BaseRetriever):\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_2516\\1174643080.py:18: DeprecationWarning: Retrievers must implement abstract `_aget_relevant_documents` method instead of `aget_relevant_documents`\n",
      "  class DummyRetriever(BaseRetriever):\n"
     ]
    }
   ],
   "source": [
    "#SCHEMAS\n",
    "\n",
    "class MultiQueryOutput(BaseModel):\n",
    "    multiple_queries: List[str] = Field(description=\"List of individual queries derived from the input multiquery.\")\n",
    "\n",
    "class collection_classification(BaseModel):\n",
    "    collection_name : str  = Field(description = f'contain value from one of the defined category which is relatable to query') \n",
    "\n",
    "class multi_or_ambigious_classification(BaseModel):\n",
    "    query_identification : str  = Field(description = f\"contain value from one of the following : ['AMBIGIOUS', 'MULTIQUERY', 'NORMAL']\") \n",
    "\n",
    "class RAG_State(TypedDict):\n",
    "    input : str\n",
    "    output : str\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-4o-mini', api_key=\"\")\n",
    "\n",
    "class DummyRetriever(BaseRetriever):\n",
    "    def get_relevant_documents(self, query): return []\n",
    "    async def aget_relevant_documents(self, query): return []\n",
    "\n",
    "mqr = MultiQueryRetriever.from_llm(retriever=DummyRetriever(), llm=llm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ff7bac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#identifying collection to search relevant vector store\n",
    "\n",
    "\n",
    "def retrival(query : str) :\n",
    "    print(f\"\\n\\nRETRIEVAL FUNCTION CALLED....\\n\\n\")\n",
    "    res= discover_collections(BASE_DIR)\n",
    "    print(res)\n",
    "\n",
    "    classification = llm.with_structured_output(collection_classification).invoke([HumanMessage(content = category_classification_system_prompt.format(res = res, query = query))])\n",
    "\n",
    "    print(F'\\nCLASSIFIED CATEGORY : {classification.collection_name}\\n')\n",
    "\n",
    "    result = main(query,classification.collection_name)\n",
    "\n",
    "    print(f\"\\nüîé Query Results ({result}):\")\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79dd35fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:4: SyntaxWarning: invalid escape sequence '\\M'\n",
      "<>:4: SyntaxWarning: invalid escape sequence '\\M'\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_2516\\1824605591.py:4: SyntaxWarning: invalid escape sequence '\\M'\n",
      "  print(f\"\\n\\MULTI QUERY SPLITTER FUNCTION CALLED....\\n\\n\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Function to split queries using LLM\n",
    "def split_multiquery_llm_and_return_output(query: str) -> List[str]:\n",
    "\n",
    "    print(f\"\\n\\MULTI QUERY SPLITTER FUNCTION CALLED....\\n\\n\")\n",
    "\n",
    "    structured_llm = llm.with_structured_output(MultiQueryOutput)\n",
    "\n",
    "    multiple_query_list = structured_llm.invoke(\n",
    "        [HumanMessage(content=multiquery_splitter_system_prompt.format(query=query))]\n",
    "    )\n",
    "    print(f\"\\n\\nMULTI QUERY SPLITTED INTO -({multiple_query_list})\\n\\n\")\n",
    "\n",
    "    list_output = []\n",
    "    for q in multiple_query_list.multiple_queries:\n",
    "        print(f\"\\n\\nsearching for query......'{q}'\")\n",
    "        output = retrival(q)\n",
    "        list_output.append(output)\n",
    "                \n",
    "    print(f\"\\n\\nMULTI QUERY OUTPUT -({list_output})\\n\\n\")\n",
    "\n",
    "    return list_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bf60b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#classifying into MULTIQUERY, AMBIGUOUS, NORMAL\n",
    "\n",
    "\n",
    "def retrieval_for_multiquery_ambigious_or_normal(query : str) -> List:\n",
    "\n",
    "    print(f\"\\n\\retrieval_for_multiquery_ambigious_or_normal FUNCTION CALLED....\\n\\n\")\n",
    "\n",
    "    multiquery_ambigious_classification = llm.with_structured_output(multi_or_ambigious_classification).invoke([HumanMessage(content = multiquery_or_ambigious_system_prompt.format(query = query))])\n",
    "\n",
    "    if multiquery_ambigious_classification.query_identification == 'MULTIQUERY':\n",
    "        print('MULTIQUERY')\n",
    "\n",
    "        multi_query_result = split_multiquery_llm_and_return_output(query)\n",
    "\n",
    "        return multi_query_result\n",
    "\n",
    "    elif multiquery_ambigious_classification.query_identification == 'AMBIGIOUS':\n",
    "\n",
    "        print('AMBIGIOUS')\n",
    "\n",
    "        rewritten_queries = mqr.llm_chain.invoke({\"question\": query})\n",
    "\n",
    "        print(f\"QUERIES GEERATED FOR AMBIGIOUS QUERY : {rewritten_queries}\")\n",
    "\n",
    "        ambiguous_retrieval_list =[]\n",
    "\n",
    "        for query in rewritten_queries:\n",
    "            \n",
    "            ambiguous_retrieval_list.append(retrival(query))\n",
    "\n",
    "        return ambiguous_retrieval_list\n",
    "\n",
    "    elif multiquery_ambigious_classification.query_identification == 'NORMAL':\n",
    "\n",
    "        result = retrival(query)\n",
    "\n",
    "        return result\n",
    "\n",
    "    else :\n",
    "        return ['no data available']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec17e6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_chain(state : RAG_State) -> RAG_State:\n",
    "\n",
    "    data = retrieval_for_multiquery_ambigious_or_normal(state['input'])\n",
    "\n",
    "    context = ', '.join(data)\n",
    "\n",
    "    print(f\"FINAL CONTEXT : {context}\")\n",
    "\n",
    "    rag_result = llm.invoke([HumanMessage(retrieval_argumented_generation_system_prompt.format(query = state['input'], context = context))])\n",
    "\n",
    "    print('LLM RESULT : ',rag_result.content)\n",
    "\n",
    "    return {'output' : rag_result.content}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0aa66f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_graph = StateGraph(RAG_State)\n",
    "rag_graph.add_node('rag_retrieval',rag_chain)\n",
    "rag_graph.add_edge(START,'rag_retrieval')\n",
    "rag_graph.add_edge('rag_retrieval',END)\n",
    "ready_rag_graph = rag_graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4e3fdf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "etrieval_for_multiquery_ambigious_or_normal FUNCTION CALLED....\n",
      "\n",
      "\n",
      "AMBIGIOUS\n",
      "QUERIES GEERATED FOR AMBIGIOUS QUERY : ['What does frontend development entail?', 'Can you explain the key aspects of frontend development?', 'What are the main components involved in frontend development?']\n",
      "\n",
      "\n",
      "RETRIEVAL FUNCTION CALLED....\n",
      "\n",
      "\n",
      "['abhiraj', 'agnetic_ai', 'frontend_dovelopment', 'machine_learning']\n",
      "\n",
      "CLASSIFIED CATEGORY : frontend_dovelopment\n",
      "\n",
      "Discovered collections: ['abhiraj', 'agnetic_ai', 'frontend_dovelopment', 'machine_learning']\n",
      "\n",
      "üîç Building/Updating FAISS Indexes...\n",
      "\n",
      "Loading existing FAISS index for collection: abhiraj\n",
      "‚ö†Ô∏è No documents for agnetic_ai. Skipping index creation.\n",
      "Loading existing FAISS index for collection: frontend_dovelopment\n",
      "Loading existing FAISS index for collection: machine_learning\n",
      "\n",
      "‚úÖ Summary of collections:\n",
      " - abhiraj: 1 documents indexed\n",
      " - agnetic_ai: 0 documents indexed\n",
      " - frontend_dovelopment: 6 documents indexed\n",
      " - machine_learning: 9 documents indexed\n",
      "......3 OF CHUNKS RETRIEVED FOR What does frontend development entail?\n",
      "\n",
      "üîé Query Results (What is Frontend Development? Frontend development refers to the practice of creating the visual and interactive aspects of a website or web application‚Äîthe parts that users directly interact with. It involves the structure, design, behavior, and content of everything displayed on the browser screen):\n",
      "\n",
      "\n",
      "RETRIEVAL FUNCTION CALLED....\n",
      "\n",
      "\n",
      "['abhiraj', 'agnetic_ai', 'frontend_dovelopment', 'machine_learning']\n",
      "\n",
      "CLASSIFIED CATEGORY : frontend_dovelopment\n",
      "\n",
      "Discovered collections: ['abhiraj', 'agnetic_ai', 'frontend_dovelopment', 'machine_learning']\n",
      "\n",
      "üîç Building/Updating FAISS Indexes...\n",
      "\n",
      "Loading existing FAISS index for collection: abhiraj\n",
      "‚ö†Ô∏è No documents for agnetic_ai. Skipping index creation.\n",
      "Loading existing FAISS index for collection: frontend_dovelopment\n",
      "Loading existing FAISS index for collection: machine_learning\n",
      "\n",
      "‚úÖ Summary of collections:\n",
      " - abhiraj: 1 documents indexed\n",
      " - agnetic_ai: 0 documents indexed\n",
      " - frontend_dovelopment: 6 documents indexed\n",
      " - machine_learning: 9 documents indexed\n",
      "......3 OF CHUNKS RETRIEVED FOR Can you explain the key aspects of frontend development?\n",
      "\n",
      "üîé Query Results (What is Frontend Development? Frontend development refers to the practice of creating the visual and interactive aspects of a website or web application‚Äîthe parts that users directly interact with. It involves the structure, design, behavior, and content of everything displayed on the browser screen):\n",
      "\n",
      "\n",
      "RETRIEVAL FUNCTION CALLED....\n",
      "\n",
      "\n",
      "['abhiraj', 'agnetic_ai', 'frontend_dovelopment', 'machine_learning']\n",
      "\n",
      "CLASSIFIED CATEGORY : frontend_dovelopment\n",
      "\n",
      "Discovered collections: ['abhiraj', 'agnetic_ai', 'frontend_dovelopment', 'machine_learning']\n",
      "\n",
      "üîç Building/Updating FAISS Indexes...\n",
      "\n",
      "Loading existing FAISS index for collection: abhiraj\n",
      "‚ö†Ô∏è No documents for agnetic_ai. Skipping index creation.\n",
      "Loading existing FAISS index for collection: frontend_dovelopment\n",
      "Loading existing FAISS index for collection: machine_learning\n",
      "\n",
      "‚úÖ Summary of collections:\n",
      " - abhiraj: 1 documents indexed\n",
      " - agnetic_ai: 0 documents indexed\n",
      " - frontend_dovelopment: 6 documents indexed\n",
      " - machine_learning: 9 documents indexed\n",
      "......3 OF CHUNKS RETRIEVED FOR What are the main components involved in frontend development?\n",
      "\n",
      "üîé Query Results (What is Frontend Development? Frontend development refers to the practice of creating the visual and interactive aspects of a website or web application‚Äîthe parts that users directly interact with. It involves the structure, design, behavior, and content of everything displayed on the browser screen):\n",
      "FINAL CONTEXT : What is Frontend Development? Frontend development refers to the practice of creating the visual and interactive aspects of a website or web application‚Äîthe parts that users directly interact with. It involves the structure, design, behavior, and content of everything displayed on the browser screen, What is Frontend Development? Frontend development refers to the practice of creating the visual and interactive aspects of a website or web application‚Äîthe parts that users directly interact with. It involves the structure, design, behavior, and content of everything displayed on the browser screen, What is Frontend Development? Frontend development refers to the practice of creating the visual and interactive aspects of a website or web application‚Äîthe parts that users directly interact with. It involves the structure, design, behavior, and content of everything displayed on the browser screen\n",
      "LLM RESULT :  Frontend development refers to the practice of creating the visual and interactive aspects of a website or web application‚Äîthe parts that users directly interact with. It involves the structure, design, behavior, and content of everything displayed on the browser screen.\n",
      "Frontend development refers to the practice of creating the visual and interactive aspects of a website or web application‚Äîthe parts that users directly interact with. It involves the structure, design, behavior, and content of everything displayed on the browser screen.\n"
     ]
    }
   ],
   "source": [
    "query = {\"input\":'what is frontend developmeeeement is','output':None}\n",
    "\n",
    "output = ready_rag_graph.invoke(query)\n",
    "\n",
    "print(output['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6c091c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
